{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0otu_2YUmt1H"
      },
      "outputs": [],
      "source": [
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "unmasker = pipeline('fill-mask', model='roberta-base')\n",
        "unmasker(\"Hello I'm a <mask> model.\")"
      ],
      "metadata": {
        "id": "UAowpKMKm32R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import RobertaTokenizer, RobertaModel\n",
        "tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
        "model = RobertaModel.from_pretrained('roberta-base')\n",
        "text = '''Sanoat, deb xom ashyo, material, yoqilgʻi, energiya va boshqa mahsulotlar ishlab chiqaruvchi yoki aholiga xizmat koʻrsatuvchi korxonalar (zavod, fabrika, kon, shaxta, elektr stansiya, ferma va hk) majmuasiga aytiladi. Sanoat xalq xoʻjaligining muhim sohasidir.\n",
        "Chet ellarda Sanoatning rivojlanishi sanoat jihatidan rivojlangan mamlakatlarning paydo boʻlishiga olib keldi. Bu mamlakatlarda iqtisodiyot taraqqiyoti katta hajmda jamgʻarilgan texnika jihatidan ilgʻor kapital asosida va mavjud yuqori malakali ishchi kuchi bilan taʼminlanadi. Ularga AQSH, Kanada, Yaponiya, koʻpgina Gʻarbiy Yevropa mamlakatlari kiradi.[1]'''\n",
        "encoded_input = tokenizer(text, return_tensors='pt')\n",
        "output = model(**encoded_input)"
      ],
      "metadata": {
        "id": "1INoqU4AnBMY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import RobertaTokenizer, TFRobertaModel\n",
        "tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
        "model = TFRobertaModel.from_pretrained('roberta-base')\n",
        "encoded_input = tokenizer(text, return_tensors='tf')\n",
        "output = model(encoded_input)"
      ],
      "metadata": {
        "id": "2bdoYvvmnlfJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "unmasker = pipeline('fill-mask', model='roberta-base')\n",
        "unmasker(\"axoliga xizmat ko'rsatadigan  <mask> joylashgan.\")"
      ],
      "metadata": {
        "id": "tNeDTnvgnvjP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import RobertaTokenizer, RobertaForMaskedLM\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "# Step 1: Preprocess the dataset\n",
        "\n",
        "# Assuming you have a text dataset stored in a file named 'text_dataset.txt'\n",
        "with open('text data.txt', 'r') as file:\n",
        "    text_data = file.read()\n",
        "\n",
        "tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
        "tokenized_text = tokenizer.tokenize(text_data)\n",
        "\n",
        "# Step 2: Prepare the training data\n",
        "\n",
        "# Split the tokenized_text into training, validation, and test sets as needed\n",
        "\n",
        "# Step 3: Install the required libraries (already assumed installed)\n",
        "\n",
        "# Step 4: Load the RoBERTa model\n",
        "model = RobertaForMaskedLM.from_pretrained('roberta-base')\n",
        "\n",
        "# Step 5: Fine-tune the model\n",
        "\n",
        "# Define a custom dataset class\n",
        "class TextDataset(Dataset):\n",
        "    def __init__(self, tokenized_text):\n",
        "        self.inputs = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.inputs)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.inputs[idx]\n",
        "\n",
        "# Create instances of the custom dataset class\n",
        "train_dataset = TextDataset(tokenized_text)\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "\n",
        "# Define the optimizer and loss function\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-5)\n",
        "loss_fn = torch.nn.CrossEntropyLoss()\n",
        "num_epochs = 20\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    for batch in train_loader:\n",
        "        inputs = batch.unsqueeze(dim=0)\n",
        "        labels = batch.unsqueeze(dim=0)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs, labels=labels)\n",
        "        loss = outputs.loss\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "    print('Epoch >',epoch)"
      ],
      "metadata": {
        "id": "c5V05QFqn6HH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 6: Evaluate the model\n",
        "\n",
        "model.eval()  # Set the model to evaluation mode\n",
        "\n",
        "validation_loss = 0.0\n",
        "total_predictions = 0\n",
        "correct_predictions = 0\n",
        "\n",
        "with torch.no_grad():\n",
        "    for batch in validation_loader:  # Assuming you have a DataLoader for the validation set\n",
        "        inputs = batch.unsqueeze(dim=0)\n",
        "        labels = batch.unsqueeze(dim=0)\n",
        "\n",
        "        outputs = model(inputs, labels=labels)\n",
        "        loss = outputs.loss\n",
        "        validation_loss += loss.item()\n",
        "\n",
        "        predicted_indices = outputs.logits.argmax(dim=-1)\n",
        "        total_predictions += len(labels)\n",
        "        correct_predictions += (predicted_indices == labels).sum().item()\n",
        "\n",
        "validation_loss /= len(validation_loader)\n",
        "accuracy = correct_predictions / total_predictions\n",
        "\n",
        "print(f\"Validation Loss: {validation_loss:.4f}\")\n",
        "print(f\"Validation Accuracy: {accuracy * 100:.2f}%\")"
      ],
      "metadata": {
        "id": "2Ql-V5c4tmoe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 7: Test the model\n",
        "\n",
        "model.eval()  # Set the model to evaluation mode\n",
        "\n",
        "test_loss = 0.0\n",
        "total_predictions = 0\n",
        "correct_predictions = 0\n",
        "\n",
        "with torch.no_grad():\n",
        "    for batch in test_loader:  # Assuming you have a DataLoader for the test set\n",
        "        inputs = batch.unsqueeze(dim=0)\n",
        "        labels = batch.unsqueeze(dim=0)\n",
        "\n",
        "        outputs = model(inputs, labels=labels)\n",
        "        loss = outputs.loss\n",
        "        test_loss += loss.item()\n",
        "\n",
        "        predicted_indices = outputs.logits.argmax(dim=-1)\n",
        "        total_predictions += len(labels)\n",
        "        correct_predictions += (predicted_indices == labels).sum().item()\n",
        "\n",
        "test_loss /= len(test_loader)\n",
        "accuracy = correct_predictions / total_predictions\n",
        "\n",
        "print(f\"Test Loss: {test_loss:.4f}\")\n",
        "print(f\"Test Accuracy: {accuracy * 100:.2f}%\")\n"
      ],
      "metadata": {
        "id": "-7ue-P0CtvH-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#using model\n",
        "from transformers import RobertaForMaskedLM, RobertaTokenizer\n",
        "\n",
        "model = RobertaForMaskedLM.from_pretrained('path_to_fine_tuned_model')\n",
        "tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
        "text = \"Amir temur  [MASK] bilan jang qilgan.\"\n",
        "\n",
        "encoded_input = tokenizer.encode_plus(text, return_tensors='pt')\n",
        "input_ids = encoded_input['input_ids']\n",
        "\n",
        "with torch.no_grad():\n",
        "    predictions = model(input_ids)[0]\n",
        "\n",
        "masked_index = torch.where(input_ids == tokenizer.mask_token_id)[1]\n",
        "predicted_token_ids = torch.argmax(predictions[0, masked_index], dim=-1)\n",
        "predicted_tokens = tokenizer.convert_ids_to_tokens(predicted_token_ids)\n",
        "\n",
        "print(f\"Predicted token: {predicted_tokens[0]}\")"
      ],
      "metadata": {
        "id": "A0fYko5Vtvz-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"Aynan Timur davrida Samarqand [MASK] bo’lib qolgan \"\n",
        "\n",
        "encoded_input = tokenizer.encode_plus(text, return_tensors='pt')\n",
        "input_ids = encoded_input['input_ids']\n",
        "\n",
        "with torch.no_grad():\n",
        "    output = model.generate(input_ids, max_length=50, num_return_sequences=5)\n",
        "\n",
        "generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "\n",
        "print(f\"Generated text: {generated_text}\")\n"
      ],
      "metadata": {
        "id": "vROqamLYuRs2"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}